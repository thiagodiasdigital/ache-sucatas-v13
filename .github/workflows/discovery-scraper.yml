name: Discovery Scrapers - Coleta Diaria

on:
  schedule:
    # Executa diariamente as 07:00 UTC (04:00 BRT)
    - cron: '0 7 * * *'

  workflow_dispatch:
    inputs:
      persist:
        description: 'Persistir no Supabase'
        type: boolean
        default: true
      scrapers:
        description: 'Scrapers a executar (virgula-separados)'
        type: string
        default: 'detran_mg'

concurrency:
  group: discovery-scrapers
  cancel-in-progress: false

env:
  PYTHON_VERSION: '3.11'
  TZ: America/Sao_Paulo

jobs:
  scrape:
    name: Coleta de Leiloes
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 supabase

      - name: Run scrapers
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          cd discovery_leiloes_gov

          PERSIST_FLAG=""
          if [ "${{ github.event.inputs.persist }}" == "true" ] || [ "${{ github.event_name }}" == "schedule" ]; then
            PERSIST_FLAG="--persist"
          fi

          SCRAPERS="${{ github.event.inputs.scrapers }}"
          SCRAPERS_FLAG=""
          if [ -n "$SCRAPERS" ]; then
            SCRAPERS_FLAG="--scrapers $SCRAPERS"
          fi

          python scrapers/run_daily.py $PERSIST_FLAG $SCRAPERS_FLAG

      - name: Upload results artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-results-${{ github.run_number }}
          path: |
            discovery_leiloes_gov/outputs/**/*.jsonl
            discovery_leiloes_gov/outputs/**/*.json
          retention-days: 30

      - name: Summary
        if: always()
        run: |
          echo "## Discovery Scrapers - Resultado" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f discovery_leiloes_gov/outputs/daily_report.json ]; then
            TOTAL=$(cat discovery_leiloes_gov/outputs/daily_report.json | python -c "import sys,json; print(json.load(sys.stdin).get('total_veiculos', 0))")
            echo "**Total de veiculos coletados:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Executado em: $(date '+%Y-%m-%d %H:%M:%S %Z')" >> $GITHUB_STEP_SUMMARY

  notify-failure:
    name: Notificar Falha por Email
    runs-on: ubuntu-latest
    needs: scrape
    if: failure()

    steps:
      - name: Log failure
        run: |
          echo "::error::Discovery Scrapers falhou!"

      - name: Send email notification
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: smtp.gmail.com
          server_port: 465
          secure: true
          username: ${{ secrets.EMAIL_ADDRESS }}
          password: ${{ secrets.EMAIL_APP_PASSWORD }}
          subject: "Discovery Scrapers - Falha na Coleta"
          to: ${{ secrets.EMAIL_ADDRESS }}
          from: Discovery Scrapers <${{ secrets.EMAIL_ADDRESS }}>
          body: |
            O workflow Discovery Scrapers falhou!

            ----------------------------------------
            DETALHES
            ----------------------------------------

            Repositorio: ${{ github.repository }}
            Branch: ${{ github.ref_name }}
            Evento: ${{ github.event_name }}

            ----------------------------------------
            ACAO NECESSARIA
            ----------------------------------------

            Verifique os logs em:
            https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}

            ----------------------------------------
            Enviado automaticamente pelo GitHub Actions
